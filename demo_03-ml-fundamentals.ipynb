{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 3 Demo: ML Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"code\"))\n",
    "from plotting_functions import *\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = os.path.join(\"data/\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "Let's bring back King County housing sale prediction data from the course introduction video. You can download the data from [here](https://www.kaggle.com/harlfoxem/housesalesprediction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(DATA_DIR + 'kc_house_data.csv')\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we are considering is:\n",
    "\n",
    "**Given a particular property in King County with some features, how should we assess that property's value (price) ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is this a classification problem or a regression problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many data points do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the columns in the dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some features. Let's try the `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to keep all the columns? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Discuss the following questions in your group\n",
    "\n",
    "- Which columns should we keep and which ones should we drop? Why?\n",
    "- Fill out this table in Markdown with your group:\n",
    "\n",
    "| Feature       | Keep or Discard | Why? |\n",
    "|---------------|-----------------|------|\n",
    "| bathrooms     |                 |      |\n",
    "| bedrooms      |                 |      |\n",
    "| condition     |                 |      |\n",
    "| date          |                 |      |\n",
    "| floors        |                 |      |\n",
    "| grade         |                 |      |\n",
    "| id            |                 |      |\n",
    "| lat           |                 |      |\n",
    "| long          |                 |      |\n",
    "| price         |                 |      |\n",
    "| sqft_above    |                 |      |\n",
    "| sqft_basement |                 |      |\n",
    "| sqft_living   |                 |      |\n",
    "| sqft_living15 |                 |      |\n",
    "| sqft_lot      |                 |      |\n",
    "| sqft_lot15    |                 |      |\n",
    "| view          |                 |      |\n",
    "| waterfront    |                 |      |\n",
    "| yr_built      |                 |      |\n",
    "| yr_renovated  |                 |      |\n",
    "| zipcode       |                 |      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['zipcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(['20141013T000000', '20141209T000000', '20150218T000000'], format='%Y%m%dT%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the value counts of the `waterfront` feature? \n",
    "housing_df['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the value_counts of `yr_renovated` feature? \n",
    "housing_df['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many opportunities to clean the data but we'll stop here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_df.drop(columns = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing_df['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DummyRegressor model \n",
    "\n",
    "from sklearn.dummy import DummyRegressor # Import DummyRegressor \n",
    "\n",
    "# Create a class object for the sklearn model.\n",
    "dummy_regr = \n",
    "\n",
    "\n",
    "# fit the dummy regressor\n",
    "\n",
    "\n",
    "# score the model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret the score here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on X using the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree model \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor # Import DecisionTreeRegressor \n",
    "\n",
    "# Create a class object for the sklearn model.\n",
    "dt_regr = \n",
    "\n",
    "\n",
    "# fit the decision tree regressor \n",
    "\n",
    "\n",
    "# score the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Discuss the following questions in your group\n",
    "\n",
    "- Should we be happy with this model and deploy it? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the depth of this model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data and  \n",
    "- Train on the train split \n",
    "- Score on the test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a class object \n",
    "dt = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "# Train a decision tree on X_train, y_train\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Score on the train set\n",
    "dt.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on the test set\n",
    "dt.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Discuss the following questions in your group\n",
    "\n",
    "- Why is there a large gap between train and test scores? \n",
    "- What would be the effect of increasing or decreasing `test_size`?\n",
    "- Why are we setting the `random_state`? Is it a good idea to try a bunch of values for the `random_state` and pick the one which gives the best scores? \n",
    "- Would it be possible to further improve the scores? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization \n",
    "\n",
    "Let's try out different tree depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth= 1 \n",
    "dt = DecisionTreeRegressor(max_depth=1, random_state=123) \n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your decision stump\n",
    "from sklearn.tree import plot_tree \n",
    "plot_tree(dt, feature_names = X.columns.tolist(), impurity=False, filled=True, fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_train, y_train) # Score on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_test, y_test) # Score on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do these scores compare to the previous scores? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try depth 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=10, random_state=123) # max_depth= 10 \n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_train, y_train) # Score on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_test, y_test) # Score on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any improvements? Which depth should we pick? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the test data again and again. How about creating a validation set to pick the right depth and assessing the final model on the test set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set \n",
    "X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_scores = []\n",
    "valid_scores = []\n",
    "depths = np.arange(1, 35, 2)\n",
    "\n",
    "for depth in depths:  \n",
    "    # Create and fit a decision tree model for the given depth  \n",
    "    dt = DecisionTreeRegressor(max_depth=depth, random_state=123)\n",
    "    \n",
    "    # Calculate and append r2 scores on the training and validation sets\n",
    "\n",
    "    \n",
    "results_single_valid_df = pd.DataFrame({\"train_score\": tr_scores, \n",
    "                           \"valid_score\": valid_scores},index = depths)\n",
    "results_single_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_single_valid_df[['train_score', 'valid_score']].plot(ylabel='r2 scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What depth gives the \"best\" validation score? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What depth gives the \"best\" validation score?\n",
    "best_depth = results_single_valid_df['valid_score'].idxmax() \n",
    "best_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(1, 35, 2)\n",
    "\n",
    "cv_train_scores = []\n",
    "cv_valid_scores = []\n",
    "for depth in depths: \n",
    "    # Create and fit a decision tree model for the given depth   \n",
    "    dt = DecisionTreeRegressor(max_depth = depth, random_state=123)\n",
    "\n",
    "    # Carry out cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"train_score\": cv_train_scores, \n",
    "                           \"valid_score\": cv_valid_scores\n",
    "                           },\n",
    "                           index=depths\n",
    "                            )\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['train_score', 'valid_score']].plot(ylabel='r2 score', title='Housing price prediction depth vs. r2 score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the \"best\" depth with cross-validation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = results_df['valid_score'].idxmax()\n",
    "best_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the following questions in your group\n",
    "\n",
    "1.\tAt which depth(s) are we underfitting? At which depth(s) are we overfitting?\n",
    "2.\tAbove, we chose the depth that gives us the best cross-validation score. Is it always a good idea to select this depth? What if a simpler model with a smaller max_depth gives nearly the same cross-validation score?\n",
    "3.\tIf our main concern is test scores, why don't we use the test set during training?\n",
    "4.\tDo you trust our hyperparameter optimization process? In other words, do you believe we've found the best possible depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model with the best depth of the full training data\n",
    "dt_final = DecisionTreeRegressor(max_depth=best_depth, random_state=123)\n",
    "dt_final.fit(X_train, y_train)\n",
    "dt_final.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these scores compare to the scores when we used a single validation set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's the depth of the model? \n",
    "dt_final.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tree(dt_final, feature_names = X_train.columns.tolist(), impurity=False, filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are the most important ones?\n",
    "dt_final.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( \n",
    "    data = {\n",
    "        \"features\": dt_final.feature_names_in_,\n",
    "        \"feature_importances\": dt_final.feature_importances_\n",
    "    }\n",
    ")\n",
    "df.sort_values(\"feature_importances\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts we revised in this demo\n",
    "\n",
    "- Exploratory data analysis\n",
    "- Baselines\n",
    "- Data splitting: train, test, validation sets\n",
    "- Cross validation\n",
    "- Underfitting, overfitting, the fundamental tradeoff\n",
    "- The golden rule of supervised ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical steps to build a supervised machine learning model\n",
    "\n",
    "- Ensure the data is appropriate for your task (e.g., labeled data, suitable features).\n",
    "- Split the data into training and testing sets.\n",
    "- Perform exploratory data analysis (EDA) on the training data to understand distributions, identify patterns, and detect potential issues.\n",
    "- Preprocess and encode features (e.g., handle missing values, scale features, encode categorical variables).\n",
    "    - coming up \n",
    "- Build a baseline model to establish a performance benchmark.\n",
    "- Train multiple candidate models on the training data.\n",
    "    - coming up  \n",
    "- Select promising models and perform hyperparameter tuning using cross-validation.\n",
    "    - coming up \n",
    "- Evaluate the generalization performance of the best model on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
